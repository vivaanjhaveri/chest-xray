{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivaanjhaveri/chest-xray/blob/main/chest_xray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPurJVTG_hb9"
      },
      "source": [
        "# Chest XRay Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxP_TsWb_oTT"
      },
      "source": [
        "### UBC Medicine Datathon 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt2DEsotOE5v"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR5L_uEmNWuS"
      },
      "source": [
        "### Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WBRjy01MvRhX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import random\n",
        "\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import HTML\n",
        "\n",
        "sys.path.append(os.path.join(os.path.abspath(\"..\"), \"code\"))\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from ipywidgets import interact, interactive\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y57HO7-L3az"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKLj4wWhNk6Q"
      },
      "source": [
        "### 1. Overview of the Dataset\n",
        "   The dataset contains metadata about chest X‑ray images. Key columns include:\n",
        "\n",
        "- **Image Index:** Identifier or filename of the image.\n",
        "- **Finding Labels:** One or more findings per image (separated by `|`).\n",
        "- **Patient Age:** Age of the patient.\n",
        "- **Patient Gender:** Gender of the patient.\n",
        "- And additional clinical information.\n",
        "\n",
        "The cells below display the first few rows and provide summary statistics for further exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYCe0maQVbBl"
      },
      "source": [
        "- **BBox_list_2017.csv:** Contains bounding box coordinates for regions of interest in the images.\n",
        "- **Data_Entry_2017.csv:** Contains class labels and additional patient metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APcZ0JYsVtt7",
        "outputId": "82192d05-6ea4-47a8-a5b6-135b1892b0bd"
      },
      "outputs": [],
      "source": [
        "# bbox_df = pd.read_csv('/BBox_List_2017.csv')\n",
        "# data_df = pd.read_csv('/Data_Entry_2017.csv')\n",
        "\n",
        "# # Display the shape of the datasets\n",
        "# print('BBox_list_2017.csv shape:', bbox_df.shape)\n",
        "# print('Data_Entry_2017.csv shape:', data_df.shape)\n",
        "\n",
        "# Display the first few rows of each dataset\n",
        "# print('\\nFirst five rows of BBox_list_2017.csv:')\n",
        "# print(bbox_df.head())\n",
        "\n",
        "# print('\\nFirst five rows of Data_Entry_2017.csv:')\n",
        "# print(data_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v75XgnlL3CY",
        "outputId": "dcd01c53-a448-49c0-dd15-46dacafb2652"
      },
      "outputs": [],
      "source": [
        "# Inspect dataset info and missing values\n",
        "# print('--- BBox_list_2017.csv Info ---')\n",
        "# print(bbox_df.info())\n",
        "# print('Missing values in BBox_list_2017.csv:')\n",
        "# print(bbox_df.isnull().sum())\n",
        "\n",
        "# print('\\n--- Data_Entry_2017.csv Info ---')\n",
        "# print(data_df.info())\n",
        "# print('Missing values in Data_Entry_2017.csv:')\n",
        "# print(data_df.isnull().sum())\n",
        "\n",
        "# # Analyze the distribution of disease classes in Data_Entry_2017\n",
        "# class_counts = data_df['Finding Labels'].value_counts()\n",
        "# print('\\nDistribution of Disease Classes:')\n",
        "# print(class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1SgjajGWFdu",
        "outputId": "393dee90-8523-42e8-a349-3921f22e3ba4"
      },
      "outputs": [],
      "source": [
        "# # Step 4: Merge the two datasets on the \"Image Index\" column\n",
        "# merged_df = pd.merge(data_df, bbox_df, on='Image Index', how='left')\n",
        "\n",
        "# # Display the shape of the merged dataframe\n",
        "# print('Merged DataFrame shape:', merged_df.shape)\n",
        "\n",
        "# # Display the first few rows of the merged dataframe\n",
        "# print(merged_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTc---q2Mj3W"
      },
      "source": [
        "### 2. Analysis of Finding Labels:\n",
        "In this section, we split the `Finding Labels` column (which may contain multiple labels separated by `|`), count the frequency of each finding, and then visualize the distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbSTd8WgMkQt",
        "outputId": "aca3fdba-5acc-405b-ed6d-001cc52602ce"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# # Split the 'Finding Labels' column and explode the list into separate rows\n",
        "# all_labels = metadata['Finding Labels'].str.split('|').explode()\n",
        "\n",
        "# # Count the occurrences of each label\n",
        "# label_counts = Counter(all_labels)\n",
        "\n",
        "# # Convert the counts to a DataFrame for visualization\n",
        "# labels_df = pd.DataFrame.from_dict(label_counts, orient='index', columns=['Count'])\n",
        "# labels_df = labels_df.sort_values(by='Count', ascending=False)\n",
        "\n",
        "# # print(\"Top 10 Findings:\")\n",
        "# print(labels_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qECy5axNvyB"
      },
      "source": [
        "## 3. Demographic Analysis\n",
        "\n",
        "Next, we examine the patient demographics. We will visualize:\n",
        "\n",
        "- **Patient Age Distribution:** Using a histogram with KDE.\n",
        "- **Patient Gender Distribution:** Using a count plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "K4XgzMZyMngu",
        "outputId": "2cf4bd54-f6e2-4a31-cc5a-fef56a3766e9"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(12, 6))\n",
        "# sns.barplot(x=labels_df.index, y=labels_df['Count'], palette='viridis')\n",
        "# plt.xticks(rotation=45, ha='right')\n",
        "# plt.title('Distribution of Findings in NIH Chest X‑ray Dataset')\n",
        "# plt.xlabel('Finding')\n",
        "# plt.ylabel('Count')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "FYoamlidMyJw",
        "outputId": "5d999a95-1904-458c-9e89-308a0540279a"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(8, 4))\n",
        "# sns.histplot(metadata['Patient Age'].dropna(), kde=True, bins=30)\n",
        "# plt.title('Distribution of Patient Age')\n",
        "# plt.xlabel('Age')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "P3w5LxcdM0aM",
        "outputId": "f76192a8-46a7-4b4f-aab6-9355ecb89634"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(6, 4))\n",
        "# sns.countplot(x='Patient Gender', data=metadata, palette='Set2')\n",
        "# plt.title('Patient Gender Distribution')\n",
        "# plt.xlabel('Gender')\n",
        "# plt.ylabel('Count')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFLfYMD3M3vf"
      },
      "source": [
        "## 4. Co-occurrence Analysis of Findings\n",
        "\n",
        "In this section, we compute a co-occurrence matrix to analyze how often different findings appear together in the dataset. This helps to understand relationships between various pathological findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "-7YZYvhqM3h_",
        "outputId": "a5d1adde-4a4e-4a50-9a75-d2446073d643"
      },
      "outputs": [],
      "source": [
        "# import itertools\n",
        "\n",
        "# # Get a sorted list of unique findings\n",
        "# unique_findings = sorted(list(all_labels.unique()))\n",
        "\n",
        "# # Initialize a DataFrame for the co-occurrence matrix\n",
        "# co_occurrence = pd.DataFrame(0, index=unique_findings, columns=unique_findings)\n",
        "\n",
        "# # Populate the matrix by iterating over each record\n",
        "# for labels in metadata['Finding Labels'].dropna():\n",
        "#     label_list = labels.split('|')\n",
        "#     # Update counts for each combination of findings\n",
        "#     for label1, label2 in itertools.combinations(label_list, 2):\n",
        "#         co_occurrence.loc[label1, label2] += 1\n",
        "#         co_occurrence.loc[label2, label1] += 1\n",
        "#     # Also increment self-occurrence for each label\n",
        "#     for label in label_list:\n",
        "#         co_occurrence.loc[label, label] += 1\n",
        "\n",
        "# # Plot the co-occurrence heatmap\n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.heatmap(co_occurrence, cmap='viridis', linewidths=0.5)\n",
        "# plt.title('Co-occurrence Matrix of Findings')\n",
        "# plt.xlabel('Finding')\n",
        "# plt.ylabel('Finding')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxI1-GNzO3ZQ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw70BQZuSk_b"
      },
      "source": [
        "## Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXsoN2WhSnRP"
      },
      "source": [
        "In this section, we will create a train-test split and build a classification task. For demonstration purposes, we will use a simplified binary classification:\n",
        "\n",
        "- **Normal vs. Abnormal:**  \n",
        "  We define images with the label \"No Finding\" as *Normal* (0) and those with any other findings as *Abnormal* (1).\n",
        "\n",
        "We will use patient demographics (e.g., Age and Gender) as features. Note that in practice you would typically extract image features from the actual chest X‑ray images, but here we use available metadata for demonstration.\n",
        "\n",
        "We will then train:\n",
        "- A baseline model using a `DummyClassifier`.\n",
        "- A Support Vector Machine (SVM) classifier with an RBF kernel.\n",
        "- Hyperparameter tuning will be performed using GridSearchCV or RandomizedSearchCV.\n",
        "\n",
        "Finally, we compare both models using classification metrics such as accuracy, precision, recall, and F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Splitting \n",
        "\n",
        "The first thing to do is to split the dataset into two pieces (training and testing). \n",
        "\n",
        "We take our dataset and split it into training and testing data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(112120, 2048) (112120, 14)\n",
            "Epoch 1/20, Loss: 0.023209236562252045\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m  \u001b[38;5;66;03m# Adjust number of epochs based on your needs\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 61\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "csv_file = \"xray_input.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "label_cols = [\n",
        "    \"Atelectasis\", \"Consolidation\", \"Infiltration\", \"Pneumothorax\", \"Edema\",\n",
        "    \"Emphysema\", \"Fibrosis\", \"Effusion\", \"Pneumonia\", \"Pleural_Thickening\",\n",
        "    \"Cardiomegaly\", \"Nodule\", \"Mass\", \"Hernia\",\n",
        "]\n",
        "\n",
        "# Create a list of the x-columns\n",
        "x_cols = [col for col in df.columns if col.startswith(\"x\")]\n",
        "X = df[x_cols][:]\n",
        "y = df[label_cols][:]\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Convert the data to a PyTorch tensor\n",
        "X_data_tensor = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
        "\n",
        "# Create a dataset and dataloader for batch processing\n",
        "dataset = TensorDataset(X_data_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "# Define the autoencoder architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=2048, encoding_dim=256):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder: reduces dimensionality\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Decoder: reconstructs the input from the encoded representation\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, input_dim),\n",
        "            nn.Sigmoid()  # Using Sigmoid if your data is normalized between 0 and 1\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "input_dim = 2048\n",
        "encoding_dim = 256  # Adjust as desired for compression\n",
        "model = Autoencoder(input_dim, encoding_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20  # Adjust number of epochs based on your needs\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        inputs = batch[0]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# After training, use the encoder part to get the compressed representation\n",
        "with torch.no_grad():\n",
        "    X_reduced_tensor = model.encoder(X_data_tensor)\n",
        "\n",
        "print(\"Compressed representation shape:\", X_reduced_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'LinearSVM' from 'CSVM' (/Users/ethanelliotrajkumar/Downloads/chest-xray/CSVM.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCSVM\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearSVM\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and testing sets\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'LinearSVM' from 'CSVM' (/Users/ethanelliotrajkumar/Downloads/chest-xray/CSVM.py)"
          ]
        }
      ],
      "source": [
        "from CSVM import LinearSVM\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_reduced_tensor, y, test_size=0.5)\n",
        "print(\"Split the dataset into training and testing sets\")\n",
        "\n",
        "Y_train = Y_train.values\n",
        "Y_test = Y_test.values\n",
        "# Initialize and train the CSVM classifier\n",
        "classifier = LinearSVM(\n",
        "    penalty='l2',\n",
        "    loss='squared_hinge',\n",
        "    dual='auto',\n",
        "    tol=0.01,\n",
        "    C=1.0,\n",
        "    multi_class='ovr',\n",
        "    fit_intercept=True,\n",
        "    intercept_scaling=1,\n",
        "    class_weight=None,\n",
        "    verbose=0,\n",
        "    random_state=None,\n",
        "    max_iter=1000,\n",
        "    n_jobs=-1\n",
        ")\n",
        "print(\"Initialized and trained the CSVM classifier\")\n",
        "classifier.fit(X_train, Y_train)\n",
        "classifier.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
